---
layout:     post
mathjax:    true
title:      "Venice High Water prediction"
subtitle:   "Can we predict floodings in Venice using metereological data?"
date:       2020-04-25
author:     "Zanett"
header-img: 
tags:
    - Data_visualization
    - Data_scraping
    - Machine_Learning
    - Python
---

> To build a city where it is impossible to build a city is madness in itself, but to build there one of the most elegant and grandest of cities is the madness of genius. -
> Alexander Herzen


Venice has for centuries been one of the most unique and fascinating cities in the world. I've lived near Venice for 24 years now, and I've been there countless times, and with each time I fell in love with the city all over again. 

The citizens and the turists needs to however deal with the struggle of the High Water ('Acqua alta') phenomenon. 
>"High Water occurs when certain events coincide, such as:
>1. A very high tide (usually during a full or new moon).
>2. Low atmospheric pressure.
>3. A scirocco wind blowing up the narrow, shallow Adriatic Sea, which forces water into the Venetian Lagoon.

>The phenomenon is most likely to take place between late September and April, and especially in the months of November, December, and october, in that order. However, thanks to global warming, it now can occur at any time of year."

> source: [Europeforvisitors](https://europeforvisitors.com/venice/articles/acqua-alta.htm)

What we are going to do today is discover if we can predict, using the hystorical metereological data, whether High Water will occurs and the expected sea level.

## Data Scraping
There is no data science without data, so the first thing we have to do is to find our dataset. Both for excercise and because a good dataset was not available, I decided to scrape the metereological data from [Weather Underground](https://www.wunderground.com/), which we can use for personal, non-commercial purposes. I won't go over every little detail and line of code, but if you're particularly interested you can find everything in my Github [here](https://github.com/Zanett96/Venice-High-Water-prediction/blob/master/Scraper.ipynb). The libraries we're going to use are *requests* and *BeautifulSoup*. While *requests* should come installed with Anaconda3, you can install *BeautifulSoup* typing

```
conda install -c anaconda beautifulsoup4 
```

The idea is to access the content of a web page trough the requests API's get() method. Then, we desire to parse the HTML to retrieve the table containing our data. However, the data in inside a table which gets rendered by the browser, so we can't simply dig into the HTML. Let's start by installing the requests_html package. To install, simply write in your shell
```
pip install requests_html
```

After retrieving the page, we render the javascript code to obtain the complete HTML code. The code should be something like this:
```python
# Retrieve a beautifoulsoup object with the HTML of the url)
def HTMLfromURL(url):
    # create an HTML asynchornous session
    session = HTMLSession()
    #use the session object to connect to the page
    page = session.get(url)
    # return the HTTP response code
    print('HTTP response: ' + str(page.status_code))   # 200: OK; 204: NO CONTENT
    # Run JavaScript code on webpage (sleep is for the loading time of the contents)
    page.html.render(sleep=4.5)

    # create a bs4 object over the html of the page
    soup = BeautifulSoup(page.html.html, 'html.parser')
    return soup
```
