---
layout:     post
mathjax:    true
title:      "Venice High Water prediction"
subtitle:   "Can we predict floodings in Venice using metereological data?"
date:       2020-04-25
author:     "Zanett"
header-img: 
tags:
    - Data_visualization
    - Data_scraping
    - Machine_Learning
    - Python
---

> To build a city where it is impossible to build a city is madness in itself, but to build there one of the most elegant and grandest of cities is the madness of genius. -
> Alexander Herzen


Venice has for centuries been one of the most unique and fascinating cities in the world. I've lived near Venice for 24 years now, and I've been there countless times, and with each time I fell in love with the city all over again. 

The citizens and the turists needs to however deal with the struggle of the High Water ('Acqua alta') phenomenon. 
>"High Water occurs when certain events coincide, such as:
>1. A very high tide (usually during a full or new moon).
>2. Low atmospheric pressure.
>3. A scirocco wind blowing up the narrow, shallow Adriatic Sea, which forces water into the Venetian Lagoon.

>The phenomenon is most likely to take place between late September and April, and especially in the months of November, December, and october, in that order. However, thanks to global warming, it now can occur at any time of year."

> source: [Europeforvisitors](https://europeforvisitors.com/venice/articles/acqua-alta.htm)

What we are going to do today is discover if we can predict, using the hystorical metereological data, whether High Water will occurs and the expected sea level.

## Data Scraping
There is no data science without data, so the first thing we have to do is to find our dataset. Both for excercise and because a good dataset was not available, I decided to scrape the metereological data from [Weather Underground](https://www.wunderground.com/), which we can use for personal, non-commercial purposes. I won't go over every little detail and line of code, but if you're particularly interested you can find everything in my Github [here](https://github.com/Zanett96/Venice-High-Water-prediction/blob/master/Scraper.ipynb). The libraries we're going to use are *requests* and *BeautifulSoup*. While *requests* should come installed with Anaconda3, you can install *BeautifulSoup* typing

```
conda install -c anaconda beautifulsoup4 
```
I strongly encourage you to look into the documentation when using those libraries if it's your first time with them. 

The idea is to access the content of a web page trough the *requests API's get()* method. Then, we desire to parse the HTML to retrieve the table containing our data. However, the data in inside a table which gets rendered by the browser, so we can't simply dig straight away into the HTML. Let's start by installing the *requests_html* package. To install, simply write in your shell
```
pip install requests_html
```

After retrieving the page using an *HTMLSession*, we render the javascript code to obtain the complete HTML code. The code should be something like this:
```python
# Retrieve a beautifoulsoup object with the HTML of the url)
def HTMLfromURL(url):
    # create an HTML asynchornous session
    session = HTMLSession()
    #use the session object to connect to the page
    page = session.get(url)
    # return the HTTP response code
    print('HTTP response: ' + str(page.status_code))   # 200: OK; 204: NO CONTENT
    # Run JavaScript code on webpage (sleep is for the loading time of the contents)
    page.html.render(sleep=4.5)

    # create a bs4 object over the html of the page
    soup = BeautifulSoup(page.html.html, 'html.parser')
    return soup
```

By inspecting the web page's HTML, we locate our target data over the table with class *'days ng-star-inserted'*.  After retrieving the table, we can easily access the headings and the body of our table. We can notice that there's an inner table, so we locate the HTML and estract the data from the various rows. We also procees to split the multi-rows which contains 'Max', 'Avg' and 'Min' of various features, and we reallocate them as new features. I preferred to do this manually because extracting from the <td> ws a bit tricky for this table. 
    
I won't post the simple slicing function, but the overall code will be as follows:
```python
def dataRetrieval(soup):
    #retrieve a bs4 object containing our table 
    weather_table = soup.find('table', class_='days ng-star-inserted')
    #retrieve the headings of the table
    weather_table_head = weather_table.thead.find_all("tr")

    # Get all the headings
    headings = []
    for td in weather_table_head[0].find_all("td"):
        headings.append(td.text)
    #print(headings)


    ## retrieve the data from the table
    column, data = [], []
    weather_table_data = weather_table.tbody.find_all('table') # find inner tables
    for table in weather_table_data:
        weather_table_rows = table.find_all('tr') # find rows
        for row in weather_table_rows:  
            column.append(row.text.strip()) #take data without spaces 
        data.append(column)
        column = []
        
    # slice the triple rows into sub-wors
    datas = slicing(data, column, headings)
    return datas
```

Since every table store one month's data, we'll need lots of them. The more the better. We start from 1997 (the first weather data available on the site for venice) and go up to 2018 (this is due to the water level data which goes up to 2018). We proceed to store all of our data in a pandas DataFrame. This process can be automated as follows:
```python
## double for - retrieve the various year's data
first = True
for year in range (1997, 2019):
    for month in range (1, 13):
        print('Retrieving weather data of '+str(month)+'/'+str(year))
        # URL of the page we want to retrieve
        url='https://www.wunderground.com/history/monthly/it/venice/date/'+str(year)+'-'+str(month)
        # Retrieve the HTML form the url
        soup = await asyncHTMLfromURL(url)
        # retrieve the data from the HTML
        fulldata = dataRetrieval(soup)
        # reformat the data section to be similar to the water level datas
        dataReformat(fulldata, year, month)
        dataframe = np.array([item[1:] for item in fulldata]).transpose()
        if first:
            df = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))
            first = False
        else:
            df2 = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))
            df = df.append(df2, ignore_index=True)
        print('weather data of '+str(month)+'/'+str(year)+' retrieved successfully!')
```
We also reformat the data in a more suitable way in the process. We can then save our dataframe so that we don't actually have to download it every single time. I'm going to use a .CSV format which is pretty standard way to save a dataset.
