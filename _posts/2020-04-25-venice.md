---
layout:     post
mathjax:    true
title:      "Venice High Water prediction"
subtitle:   "Can we predict floodings in Venice using metereological data?"
date:       2020-04-25
author:     "Zanett"
header-img: 
tags:
    - Data_visualization
    - Data_scraping
    - Machine_Learning
    - Python
---

> To build a city where it is impossible to build a city is madness in itself, but to build there one of the most elegant and grandest of cities is the madness of genius. -
> Alexander Herzen


Venice has for centuries been one of the most unique and fascinating cities in the world. I've lived near Venice for 24 years now and I've been there countless times, and with each time I fell in love all over again. 

The citizens and the turists needs to however deal with the struggle of the High Water ('Acqua alta') phenomenon. 
>"High Water occurs when certain events coincide, such as:
>1. A very high tide (usually during a full or new moon).
>2. Low atmospheric pressure.
>3. A scirocco wind blowing up the narrow, shallow Adriatic Sea, which forces water into the Venetian Lagoon.

>The phenomenon is most likely to take place between late September and April, and especially in the months of November, December, and october, in that order. However, thanks to global warming, it now can occur at any time of year."

> source: [Europeforvisitors](https://europeforvisitors.com/venice/articles/acqua-alta.htm)

What we are going to do today is discover if we can predict, using the hystorical metereological data, whether High Water will occurs and the expected sea level.

## Data Scraping
There is no data science without data, so the first thing we have to do is to find our dataset. Both for excercise and because a good dataset was not available, I decided to scrape the metereological data from [Weather Underground](https://www.wunderground.com/), which we can use for personal, non-commercial purposes. I won't go over every little detail and line of code, but if you're particularly interested you can find everything in my Github [here](https://github.com/Zanett96/Venice-High-Water-prediction/blob/master/Scraper.ipynb). The libraries we're going to use are *requests* and *BeautifulSoup*. While *requests* should come installed with Anaconda3, you can install *BeautifulSoup* typing

```
conda install -c anaconda beautifulsoup4 
```
I strongly encourage you to look into the documentation when using those libraries if it's your first time with them. 

The idea is to access the content of a web page trough the *requests API's get()* method. Then, we desire to parse the HTML to retrieve the table containing our data. However, the data in inside a table which gets rendered by the browser, so we can't simply dig straight away into the HTML. Let's start by installing the *requests_html* package. To install, simply write in your shell
```
pip install requests_html
```

After retrieving the page using an *HTMLSession*, we render the javascript code to obtain the complete HTML code. The code should be something like this:
```python
# Retrieve a beautifoulsoup object with the HTML of the url)
def HTMLfromURL(url):
    # create an HTML asynchornous session
    session = HTMLSession()
    #use the session object to connect to the page
    page = session.get(url)
    # return the HTTP response code
    print('HTTP response: ' + str(page.status_code))   # 200: OK; 204: NO CONTENT
    # Run JavaScript code on webpage (sleep is for the loading time of the contents)
    page.html.render(sleep=4.5)

    # create a bs4 object over the html of the page
    soup = BeautifulSoup(page.html.html, 'html.parser')
    return soup
```

By inspecting the web page's HTML, we locate our target data over the table with class *'days ng-star-inserted'*.  After retrieving the table, we can easily access the headings and the body of our table. We can notice that there's an inner table, so we locate the HTML and estract the data from the various rows. We also procees to split the multi-rows which contains 'Max', 'Avg' and 'Min' of various features, and we reallocate them as new features. I preferred to do this manually because extracting from the <td> ws a bit tricky for this table. 
    
I won't post the simple slicing function, but the overall code will be as follows:
```python
def dataRetrieval(soup):
    #retrieve a bs4 object containing our table 
    weather_table = soup.find('table', class_='days ng-star-inserted')
    #retrieve the headings of the table
    weather_table_head = weather_table.thead.find_all("tr")

    # Get all the headings
    headings = []
    for td in weather_table_head[0].find_all("td"):
        headings.append(td.text)
    #print(headings)


    ## retrieve the data from the table
    column, data = [], []
    weather_table_data = weather_table.tbody.find_all('table') # find inner tables
    for table in weather_table_data:
        weather_table_rows = table.find_all('tr') # find rows
        for row in weather_table_rows:  
            column.append(row.text.strip()) #take data without spaces 
        data.append(column)
        column = []
        
    # slice the triple rows into sub-wors
    datas = slicing(data, column, headings)
    return datas
```

Since every table store one month's data, we'll need lots of them. The more the better. We start from 1997 (the first weather data available on the site for venice) and go up to 2018 (this is due to the water level data which goes up to 2018). We proceed to store all of our data in a pandas DataFrame. This process can be automated as follows:
```python
## double for - retrieve the various year's data
first = True
for year in range (1997, 2019):
    for month in range (1, 13):
        print('Retrieving weather data of '+str(month)+'/'+str(year))
        # URL of the page we want to retrieve
        url='https://www.wunderground.com/history/monthly/it/venice/date/'+str(year)+'-'+str(month)
        # Retrieve the HTML form the url
        soup = await asyncHTMLfromURL(url)
        # retrieve the data from the HTML
        fulldata = dataRetrieval(soup)
        # reformat the data section to be similar to the water level datas
        dataReformat(fulldata, year, month)
        dataframe = np.array([item[1:] for item in fulldata]).transpose()
        if first:
            df = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))
            first = False
        else:
            df2 = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))
            df = df.append(df2, ignore_index=True)
        print('weather data of '+str(month)+'/'+str(year)+' retrieved successfully!')
```
We also reformat the data in a more suitable way in the process. We can then save our dataframe so that we don't actually have to download it every single time. I'm going to use a .CSV format which is pretty standard way to save a dataset.

We have successfully scraped our historical metereological data, but we are missing the key feature: the historical data for the water level in Venice.  Luckily, we can find the data we need in the following [page](https://www.comune.venezia.it/it/content/archivio-storico-livello-marea-venezia-1). We need however to download every .csv and rearrange them in a single dataset, which we'll later join with the other dataset we've generated previously. We could do this manually, but let's use some python magic to automate the procedure! 

Once again to move around the various urls we can just change the year from the url, and trough the *request.get()* function we can download every .csv which we'll then write on the disk. This is easily done by doing the following:
```python
## from every url download and save on disk the .csv 
for year in range (1997, 2019):
    url = 'https://www.comune.venezia.it/sites/comune.venezia.it/files/documenti/centro_maree/archivioDati/valoriorari_puntasalute_'+str(year)+'.csv'
    obj = requests.get(url)

    with open('./sea_level/'+str(year)+'_sea_levels.csv', 'wb') as file:
        file.write(obj.content)
```

Much quicker than doing it manually right? 
By checking our .csv, we discover that the data are arranged hourly for each day, while we only have one single weather value for every day. To play around this, we can extract the maximum, the minium and the average of the sea levels for each day. (Note that the 2015 files has some rows that contains plain text at the end that needs to be removed!)
To load the csv we can use Pandas. Since the file use the ';' as separator, we need to specify it (since pandas usually have ',' as default).

The idea now is to cycle trough every year, extract the info we need for every set of days and re-arrange everything in a new dataset. The data column name change from 2016 onwards from 'Data' to 'data' and 'GIORNO', so keep that in mind while extracting the data. Using functions will make the code cleaner, so don't just paste the same code over and over! Moreover, the year 2018 has metres as a metric, so we'll adapt it to cm as the rest of the dataset. Since the code is a bit heavy, I won't post it here, but you can find it as always on Github. Finally, we save our new dataset as a .csv once again.

Now that we have our datasets, we can load them and join them as a single dataset using the 'day' value. 

## Data analysis

We can inspect some rows of our dataset using the *head()* method of Pandas.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dates</th>
      <th>Temperature (° F) Max</th>
      <th>Temperature (° F) Avg</th>
      <th>Temperature (° F) Min</th>
      <th>Dew Point (° F) Max</th>
      <th>Dew Point (° F) Avg</th>
      <th>Dew Point (° F) Min</th>
      <th>Humidity (%) Max</th>
      <th>Humidity (%) Avg</th>
      <th>Humidity (%) Min</th>
      <th>Wind Speed (mph) Max</th>
      <th>Wind Speed (mph) Avg</th>
      <th>Wind Speed (mph) Min</th>
      <th>Pressure (Hg) Max</th>
      <th>Pressure (Hg) Avg</th>
      <th>Pressure (Hg) Min</th>
      <th>Precipitation (in) Total</th>
      <th>min_sea_level (cm)</th>
      <th>avg_sea_level (cm)</th>
      <th>max_sea_level (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>01/01/1997</td>
      <td>41</td>
      <td>37.4</td>
      <td>34</td>
      <td>34</td>
      <td>10.3</td>
      <td>0</td>
      <td>87</td>
      <td>80.0</td>
      <td>75</td>
      <td>23</td>
      <td>5.2</td>
      <td>0</td>
      <td>30.1</td>
      <td>29.9</td>
      <td>29.8</td>
      <td>0.0</td>
      <td>6.0</td>
      <td>56.70</td>
      <td>87.0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>02/01/1997</td>
      <td>39</td>
      <td>37.2</td>
      <td>34</td>
      <td>36</td>
      <td>31.2</td>
      <td>0</td>
      <td>87</td>
      <td>84.0</td>
      <td>81</td>
      <td>13</td>
      <td>4.5</td>
      <td>0</td>
      <td>30.1</td>
      <td>30.0</td>
      <td>29.9</td>
      <td>0.0</td>
      <td>16.0</td>
      <td>41.58</td>
      <td>82.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>03/01/1997</td>
      <td>43</td>
      <td>39.5</td>
      <td>0</td>
      <td>37</td>
      <td>35.0</td>
      <td>0</td>
      <td>87</td>
      <td>80.1</td>
      <td>0</td>
      <td>13</td>
      <td>6.8</td>
      <td>0</td>
      <td>29.9</td>
      <td>28.7</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>17.0</td>
      <td>51.58</td>
      <td>104.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>04/01/1997</td>
      <td>45</td>
      <td>40.7</td>
      <td>36</td>
      <td>41</td>
      <td>35.2</td>
      <td>0</td>
      <td>100</td>
      <td>84.4</td>
      <td>81</td>
      <td>17</td>
      <td>6.1</td>
      <td>0</td>
      <td>29.6</td>
      <td>29.6</td>
      <td>29.5</td>
      <td>0.0</td>
      <td>19.0</td>
      <td>62.75</td>
      <td>120.0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>05/01/1997</td>
      <td>43</td>
      <td>40.4</td>
      <td>39</td>
      <td>37</td>
      <td>35.8</td>
      <td>34</td>
      <td>87</td>
      <td>82.9</td>
      <td>81</td>
      <td>7</td>
      <td>1.7</td>
      <td>0</td>
      <td>29.9</td>
      <td>29.7</td>
      <td>29.6</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>50.67</td>
      <td>95.0</td>
    </tr>
  </tbody>
</table>



Let's take a peek at our data finally. We have 20 columns in total. Let's go over them quickly: 

1. **Dates**: This is the day where the measures takes place. It will surely be useful due to many information indirectly stored into it. As is its stored now it's hard to use properly, so I'll probably split the date into 3 columns: day, month and year.
2. **Temperature**: This is a daily measure of the temperature in Fahrenit. To make up to the fact that we need to analyze an entire day, which involves many temperature changes, we use maximum, minimum and average temperature of each day. 
3. **Dew Point**: The dew point represents the temperature to which air must be cooled to in order to reach saturation (assuming air pressure and moisture content are constant). We consider the maximum, the minimum and the average. 
4. **Humidity**: Humidity is the amount of water vapor in the air. Usually on the report (as in our case) what is expressed is the so-called *relative humidity*, which is the amount of water vapor actually in the air, expressed as a percentage of the maximum amount of water vapor the air can hold at the same temperature. We consider the maximum, the minimum and the average. 
5. **Wind Speed**: This is the speed of the wind measured in metres per hour. We consider the maximum, the minimum and the average. 
6. **Pressure**: This represent the air pressure, which we can identify as "the force per unit of area exerted on the Earth’s surface by the weight of the air above the surface." This is measured in inch of mercury (Hg).  Once again, we consider the maximum, the minimum and the average. 
7. **Precipitation**: this represents the millimetres of rain that have fallen. 
8. **Sea level**: This is the sea level registered at 'Punta della salute' in Venice. To help understand better this value, we defined the tide (considering measures in this particular place) as:

    1. *Sustained* if the sea level is between 80 and 109 cm above the sea;
    2. *Strongly sustained* if the sea level is between 110 and 139 cm above the sea;
    3. *Exceptional high-water* if the sea level is 140cm or more above the sea.
    
What we want to predict is whether or not there will be high-water, and the level that we'll expect from the tide. Therefore, we're in the scope of a supervised problem of binary classification and one of regression.

Since as a non-american I don't really use Fahrenheit , I'll  convert temperatures to Celusius. 

By inspecting our data with the *info()* Pandas method, we can see that we have 7938 non-null entries for the metereological data, and 7918 for the sea level. Moreover, all of our data have numerical values, minus the date.

We can also take a look at the summary of the numerical attribues by using the Pandas's *describe()* method.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Temperature (° C) Max</th>
      <th>Temperature (° C) Avg</th>
      <th>Temperature (° C) Min</th>
      <th>Dew Point (° C) Max</th>
      <th>Dew Point (° C) Avg</th>
      <th>Dew Point (° C) Min</th>
      <th>Humidity (%) Max</th>
      <th>Humidity (%) Avg</th>
      <th>Humidity (%) Min</th>
      <th>Wind Speed (mph) Max</th>
      <th>Wind Speed (mph) Avg</th>
      <th>Wind Speed (mph) Min</th>
      <th>Pressure (Hg) Max</th>
      <th>Pressure (Hg) Avg</th>
      <th>Pressure (Hg) Min</th>
      <th>Precipitation (in) Total</th>
      <th>min_sea_level (cm)</th>
      <th>avg_sea_level (cm)</th>
      <th>max_sea_level (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.000000</td>
      <td>7938.0</td>
      <td>7918.000000</td>
      <td>7918.000000</td>
      <td>7918.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>17.708491</td>
      <td>14.117109</td>
      <td>9.306248</td>
      <td>11.767070</td>
      <td>8.685941</td>
      <td>3.282313</td>
      <td>90.485135</td>
      <td>73.755455</td>
      <td>52.875535</td>
      <td>12.680146</td>
      <td>5.879617</td>
      <td>1.152683</td>
      <td>30.049811</td>
      <td>29.869337</td>
      <td>29.091119</td>
      <td>0.0</td>
      <td>-13.376358</td>
      <td>29.813988</td>
      <td>68.359939</td>
    </tr>
    <tr>
      <td>std</td>
      <td>8.482248</td>
      <td>8.007571</td>
      <td>8.569356</td>
      <td>8.017119</td>
      <td>8.281817</td>
      <td>11.581388</td>
      <td>9.413256</td>
      <td>12.189284</td>
      <td>19.048935</td>
      <td>13.314916</td>
      <td>3.193778</td>
      <td>1.866562</td>
      <td>1.145049</td>
      <td>1.526021</td>
      <td>4.923807</td>
      <td>0.0</td>
      <td>18.874075</td>
      <td>13.261333</td>
      <td>16.335222</td>
    </tr>
    <tr>
      <td>min</td>
      <td>-2.000000</td>
      <td>-12.666667</td>
      <td>-17.000000</td>
      <td>-12.000000</td>
      <td>-16.611111</td>
      <td>-96.000000</td>
      <td>44.000000</td>
      <td>12.800000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.200000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>-83.000000</td>
      <td>-14.000000</td>
      <td>11.000000</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>11.000000</td>
      <td>7.333333</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>2.902778</td>
      <td>-2.000000</td>
      <td>87.000000</td>
      <td>65.925000</td>
      <td>43.000000</td>
      <td>8.000000</td>
      <td>3.800000</td>
      <td>0.000000</td>
      <td>29.900000</td>
      <td>29.800000</td>
      <td>29.800000</td>
      <td>0.0</td>
      <td>-26.000000</td>
      <td>21.710000</td>
      <td>58.000000</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>17.000000</td>
      <td>14.388889</td>
      <td>10.000000</td>
      <td>12.000000</td>
      <td>9.777778</td>
      <td>7.000000</td>
      <td>93.000000</td>
      <td>74.200000</td>
      <td>53.000000</td>
      <td>10.000000</td>
      <td>5.400000</td>
      <td>0.000000</td>
      <td>30.100000</td>
      <td>30.000000</td>
      <td>29.900000</td>
      <td>0.0</td>
      <td>-14.000000</td>
      <td>29.120000</td>
      <td>68.000000</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>25.000000</td>
      <td>21.000000</td>
      <td>17.000000</td>
      <td>17.000000</td>
      <td>15.555556</td>
      <td>12.000000</td>
      <td>100.000000</td>
      <td>82.400000</td>
      <td>65.000000</td>
      <td>15.000000</td>
      <td>7.300000</td>
      <td>2.000000</td>
      <td>30.200000</td>
      <td>30.100000</td>
      <td>30.100000</td>
      <td>0.0</td>
      <td>-1.000000</td>
      <td>36.867500</td>
      <td>78.000000</td>
    </tr>
    <tr>
      <td>max</td>
      <td>92.000000</td>
      <td>31.055556</td>
      <td>27.000000</td>
      <td>97.000000</td>
      <td>25.555556</td>
      <td>23.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>921.000000</td>
      <td>33.100000</td>
      <td>20.000000</td>
      <td>56.800000</td>
      <td>30.800000</td>
      <td>30.600000</td>
      <td>0.0</td>
      <td>76.000000</td>
      <td>103.710000</td>
      <td>154.000000</td>
    </tr>
  </tbody>
</table>

*count* is the number of samples.  *mean* is quite obviously the mean value fo the feature, while *std* represents the standard deviation. The percentiles rows shows the value below which a given percentage of observations in a group of observations fall.

We can notice easily some things from here, i.e unrealistic data like a maximum temperature of 92° or the fact that data relatives to the precipitation are just a series of zeros. We can see that the high-water happens only <25% of the grandtotal of days recorded. 
We can also make some assumptions over the statistical attributes of the data. For example, similiar mean and std let us assume for a standard distribution.

## Data pre-processing

We can start by removing the precipitation column, since it's clearly useless. On the chance that Precipitation might actually turn useful for our analysis, I found additional data over the [National Oceanic and Atmospheric Administration](https://www.ncdc.noaa.gov/cdo-web/) (NOAA) website. We can load the dataset and join the table trough the data once again. This require some reformatting that is explained on the Notebook. In particular, we also have to do some data processing over our new feature. We can find the documentation of the data we've extracted [here](https://www7.ncdc.noaa.gov/CDO/GSOD_DESC.txt). In particular, we can see that for the precipitation feature a value of .00 indicates no measurable precipitation while 99.99 indicates missing values. There's also a flag which express additional information regarding the measure which we don't care about. We also use this chance to transform from inches to mm for more clarity.

It's better to represent the dates by separing day, month and year, since we can imagine that some patterns or relations (like season cycles, which involves different meteorological phenomena) might be hidden in it. We can just drop the composed date afterwards since it's redundant. I doubt that the 'day' column will prove useful, but we'll keep it for now and eventually drop later on. The simple dates splitter function can be found on the Notebook. 

We can create the binary column that determine whether or not a day will have high-water, which we can use as target for the binary classification and for studies later on, by using a binary flag set to 1 whenever the maximum level of high water for a day is over 80cm. 

A good way to quickly analize feature's correlation with one another is by pivoting them. This can help gain a lot of insight about previous assumptions over the data you had and create new ones. For example, we can see how the various months relates to high water phenomena. 

```python
df[['Month', 'High Water']].groupby(['Month'], as_index=True).mean().sort_values(by='High Water', ascending=False)
```

Running the previous line on our datasets outputs the following


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>High Water</th>
    </tr>
    <tr>
      <th>Month</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>11</td>
      <td>0.409091</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.287812</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.243759</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.197853</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.194190</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.189681</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.179153</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.177273</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.168622</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.134969</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.073206</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.063636</td>
    </tr>
  </tbody>
</table>

Looks pretty clear that during winter and spring, venice is more subjected to floodings. Guess splitting the date was useful after all. By doing the same operation but changing months with years, we can see that the occurance of the high water phenomena has increased substantially over the years. This is probably correlated to the climate crisis.

## Data visualization

We should always try to visualize data whenever is possible. This helps us discover statistical relations, trends or anomalies on data. One quick way to take a peek at the data is to use the *hist()* method of pandas.

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_32_0.png)

We can clearly see that some features has bell curve distribution, while others like the average wind speed or humidity are left or right skewed, which indicates that there are outliers in the data. Some histogram may appear clunky due to the fixed size for every histogram, so we'll go later in detail for every feature.

By plotting the maximum sea level over the years as a strip plot we can see how the sea level vary across the years.


![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_35_1.png)

If we use a Pie chart we can quickly see that almost 21% of the days in venice features some form of flooding.

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_37_0.png)

Out of those 20% of the days, the majority of it is just a sustained high water, (which luckily don't afctually create any problem to the citzen of Venice). Only less than the 15% is strongly sustained or expectional. To check this, we create a temporary column from which we'll count the different levels. We can then count the occurances and plot the Piechart.

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_40_0.png)

Let's check the precipitation. 

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_43_1.png)

While the data might appear heavily skewed, we need to consider that in Venice only rain around 70 days over a full year, so this result is understandable. We can also try to use a scatter plot to search for some kind of relations between the precipitation and the level of the high water.

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_45_1.png)

It doesn't appear to be a direct relation between the water level and the precipitation, however by plotting the precipitation over the water level we can see that the high water phenomena occurs mostly whenever it rains more. A particular high error is howeverobserved for the exceptional high water level.

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_47_1.png)

Let's switch our focus over the temperature data. 

![png](https://raw.githubusercontent.com/Zanett96/Zanett96.github.io/master/img/in-post/venice/output_49_2.png)
